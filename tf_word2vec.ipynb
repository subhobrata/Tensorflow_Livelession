{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_word2vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhobrata/Tensorflow_Livelession/blob/master/tf_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "VKLi6-Trikox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1040
        },
        "outputId": "91792faa-bcdc-446c-827a-a61f58513a87"
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import collections\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import datetime as dt\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def maybe_download(filename, url, expected_bytes):\n",
        "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
        "    statinfo = os.stat(filename)\n",
        "    if statinfo.st_size == expected_bytes:\n",
        "        print('Found and verified', filename)\n",
        "    else:\n",
        "        print(statinfo.st_size)\n",
        "        raise Exception(\n",
        "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
        "    return filename\n",
        "\n",
        "\n",
        "# Read the data into a list of strings.\n",
        "def read_data(filename):\n",
        "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
        "    with zipfile.ZipFile(filename) as f:\n",
        "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
        "    return data\n",
        "\n",
        "def build_dataset(words, n_words):\n",
        "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    data = list()\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "        if word in dictionary:\n",
        "            index = dictionary[word]\n",
        "        else:\n",
        "            index = 0  # dictionary['UNK']\n",
        "            unk_count += 1\n",
        "        data.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return data, count, dictionary, reversed_dictionary\n",
        "\n",
        "\n",
        "def collect_data(vocabulary_size=10000):\n",
        "    url = 'http://mattmahoney.net/dc/'\n",
        "    filename = maybe_download('text8.zip', url, 31344016)\n",
        "    vocabulary = read_data(filename)\n",
        "    print(vocabulary[:7])\n",
        "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
        "                                                                vocabulary_size)\n",
        "    del vocabulary  # Hint to reduce memory.\n",
        "    return data, count, dictionary, reverse_dictionary\n",
        "\n",
        "data_index = 0\n",
        "# generate batch data\n",
        "def generate_batch(data, batch_size, num_skips, skip_window):\n",
        "    global data_index\n",
        "    assert batch_size % num_skips == 0\n",
        "    assert num_skips <= 2 * skip_window\n",
        "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
        "    buffer = collections.deque(maxlen=span)\n",
        "    for _ in range(span):\n",
        "        buffer.append(data[data_index])\n",
        "        data_index = (data_index + 1) % len(data)\n",
        "    for i in range(batch_size // num_skips):\n",
        "        target = skip_window  # input word at the center of the buffer\n",
        "        targets_to_avoid = [skip_window]\n",
        "        for j in range(num_skips):\n",
        "            while target in targets_to_avoid:\n",
        "                target = random.randint(0, span - 1)\n",
        "            targets_to_avoid.append(target)\n",
        "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
        "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
        "        buffer.append(data[data_index])\n",
        "        data_index = (data_index + 1) % len(data)\n",
        "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
        "    data_index = (data_index + len(data) - span) % len(data)\n",
        "    return batch, context\n",
        "\n",
        "vocabulary_size = 10000\n",
        "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocabulary_size)\n",
        "\n",
        "batch_size = 128\n",
        "embedding_size = 300  # Dimension of the embedding vector.\n",
        "skip_window = 2       # How many words to consider left and right.\n",
        "num_skips = 2         # How many times to reuse an input to generate a label.\n",
        "\n",
        "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
        "# validation samples to the words that have a low numeric ID, which by\n",
        "# construction are also the most frequent.\n",
        "valid_size = 16     # Random set of words to evaluate similarity on.\n",
        "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
        "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
        "num_sampled = 64    # Number of negative examples to sample.\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data.\n",
        "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "  train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
        "\n",
        "  # Look up embeddings for inputs.\n",
        "  embeddings = tf.Variable(\n",
        "      tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "  embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
        "\n",
        "  # Construct the variables for the softmax\n",
        "  weights = tf.Variable(\n",
        "      tf.truncated_normal([embedding_size, vocabulary_size],\n",
        "                          stddev=1.0 / math.sqrt(embedding_size)))\n",
        "  biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "  hidden_out = tf.transpose(tf.matmul(tf.transpose(weights), tf.transpose(embed))) + biases\n",
        "\n",
        "  # convert train_context to a one-hot format\n",
        "  train_one_hot = tf.one_hot(train_context, vocabulary_size)\n",
        "\n",
        "  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, labels=train_one_hot))\n",
        "\n",
        "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)\n",
        "\n",
        "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
        "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
        "  normalized_embeddings = embeddings / norm\n",
        "  valid_embeddings = tf.nn.embedding_lookup(\n",
        "      normalized_embeddings, valid_dataset)\n",
        "  similarity = tf.matmul(\n",
        "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
        "\n",
        "  # Add variable initializer.\n",
        "  init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "def run(graph, num_steps):\n",
        "    with tf.Session(graph=graph) as session:\n",
        "      # We must initialize all variables before we use them.\n",
        "      init.run()\n",
        "      print('Initialized')\n",
        "\n",
        "      average_loss = 0\n",
        "      for step in range(num_steps):\n",
        "        batch_inputs, batch_context = generate_batch(data,\n",
        "            batch_size, num_skips, skip_window)\n",
        "        feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
        "\n",
        "        # We perform one update step by evaluating the optimizer op (including it\n",
        "        # in the list of returned values for session.run()\n",
        "        _, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
        "        average_loss += loss_val\n",
        "\n",
        "        if step % 2000 == 0:\n",
        "          if step > 0:\n",
        "            average_loss /= 2000\n",
        "          # The average loss is an estimate of the loss over the last 2000 batches.\n",
        "          print('Average loss at step ', step, ': ', average_loss)\n",
        "          average_loss = 0\n",
        "\n",
        "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
        "        if step % 10000 == 0:\n",
        "          sim = similarity.eval()\n",
        "          for i in range(valid_size):\n",
        "            valid_word = reverse_dictionary[valid_examples[i]]\n",
        "            top_k = 8  # number of nearest neighbors\n",
        "            nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
        "            log_str = 'Nearest to %s:' % valid_word\n",
        "            for k in range(top_k):\n",
        "              close_word = reverse_dictionary[nearest[k]]\n",
        "              log_str = '%s %s,' % (log_str, close_word)\n",
        "            print(log_str)\n",
        "      final_embeddings = normalized_embeddings.eval()\n",
        "\n",
        "num_steps = 100\n",
        "softmax_start_time = dt.datetime.now()\n",
        "run(graph, num_steps=num_steps)\n",
        "softmax_end_time = dt.datetime.now()\n",
        "print(\"Softmax method took {} minutes to run 100 iterations\".format((softmax_end_time-softmax_start_time).total_seconds()))\n",
        "\n",
        "with graph.as_default():\n",
        "\n",
        "    # Construct the variables for the NCE loss\n",
        "    nce_weights = tf.Variable(\n",
        "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
        "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "\n",
        "    nce_loss = tf.reduce_mean(\n",
        "        tf.nn.nce_loss(weights=nce_weights,\n",
        "                       biases=nce_biases,\n",
        "                       labels=train_context,\n",
        "                       inputs=embed,\n",
        "                       num_sampled=num_sampled,\n",
        "                       num_classes=vocabulary_size))\n",
        "\n",
        "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)\n",
        "\n",
        "    # Add variable initializer.\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "num_steps = 50000\n",
        "nce_start_time = dt.datetime.now()\n",
        "run(graph, num_steps)\n",
        "nce_end_time = dt.datetime.now()\n",
        "print(\"NCE method took {} minutes to run 100 iterations\".format((nce_end_time-nce_start_time).total_seconds()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found and verified text8.zip\n",
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-1-bd0351ae118b>:133: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-bd0351ae118b>:139: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "Initialized\n",
            "Average loss at step  0 :  9.270702362060547\n",
            "Nearest to during: anne, petition, asbestos, geography, pirate, percussion, broke, constellations,\n",
            "Nearest to new: hear, arctic, israelites, bone, observances, ceremony, hayek, arabic,\n",
            "Nearest to and: croatia, aka, occurrence, favor, seized, franchise, rosa, marxism,\n",
            "Nearest to not: lasers, increasing, susceptible, franks, guilt, khazar, chiefs, centre,\n",
            "Nearest to war: collect, existence, cdot, locked, bismarck, association, friction, isomorphism,\n",
            "Nearest to between: comparing, mythos, modeling, hollow, melody, basil, separated, isbn,\n",
            "Nearest to his: plateau, parallel, rounded, fascist, vii, contain, dated, tens,\n",
            "Nearest to no: normally, captivity, proportional, flash, tunisia, cost, recover, legitimate,\n",
            "Nearest to an: hampshire, project, occurring, champion, runs, twelve, interactions, ladies,\n",
            "Nearest to history: leap, constantinople, turkey, grandson, chilean, daughters, consumed, races,\n",
            "Nearest to while: thermal, bounded, anchor, batter, node, storage, dave, militant,\n",
            "Nearest to but: bristol, buffalo, divide, dedicated, iii, periodic, deposits, mb,\n",
            "Nearest to some: surroundings, kentucky, proportional, numerals, verde, frankenstein, described, seized,\n",
            "Nearest to i: authority, portal, ezekiel, dairy, reich, shopping, lanka, intellectuals,\n",
            "Nearest to these: levels, plain, unusual, surrender, photograph, participation, anonymous, hurricane,\n",
            "Nearest to has: engagement, coordinates, sit, batting, meeting, rome, exodus, theories,\n",
            "Softmax method took 28.225722 minutes to run 100 iterations\n",
            "Initialized\n",
            "Average loss at step  0 :  9.313078880310059\n",
            "Nearest to during: surfaces, doctrines, reforms, senate, broadly, migrant, attend, maternal,\n",
            "Nearest to new: jet, those, scholarly, conjugation, gif, eisenhower, optimal, hill,\n",
            "Nearest to and: val, siege, hundreds, aftermath, singular, arrives, helicopters, freed,\n",
            "Nearest to not: losing, auckland, taiwan, feels, agent, leibniz, riot, soldier,\n",
            "Nearest to war: improve, persuaded, licenses, breeding, populated, manuscripts, paint, approaching,\n",
            "Nearest to between: forests, practiced, appeared, mother, atta, resisted, sends, democratic,\n",
            "Nearest to his: duration, slightly, finland, dynasty, schism, gerald, thinking, extensions,\n",
            "Nearest to no: rod, madagascar, assistant, georgian, nevada, menu, governors, htm,\n",
            "Nearest to an: gift, license, nicaragua, many, offshore, buffer, bengals, metropolitan,\n",
            "Nearest to history: iraqi, songs, et, seeing, free, cryptanalysis, sitting, distortion,\n",
            "Nearest to while: desktop, ie, flip, portable, ca, wire, holocaust, depression,\n",
            "Nearest to but: encouraging, attained, families, brooks, executive, harbor, ancient, fruit,\n",
            "Nearest to some: genitive, gasoline, essentially, consent, enable, touch, absorb, contributors,\n",
            "Nearest to i: propulsion, offer, absolute, nigeria, across, substitute, influenced, cannot,\n",
            "Nearest to these: touch, televisions, armoured, zimbabwe, mcluhan, post, significant, lands,\n",
            "Nearest to has: dynasty, cl, bind, japanese, locations, abolished, p, followed,\n",
            "Average loss at step  2000 :  9.329561030864715\n",
            "Average loss at step  4000 :  9.318903459072112\n",
            "Average loss at step  6000 :  9.314008700370788\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}